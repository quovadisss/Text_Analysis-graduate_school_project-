{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic tokenize\n",
    "- sent_tokenize\n",
    "- word_tokenize\n",
    "- wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God is Great!', 'I won a lottery.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
    "\n",
    "sentences = \"God is Great! I won a lottery.\"\n",
    "result = sent_tokenize(sentences)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello My name is Alice I live in Seoul']\n"
     ]
    }
   ],
   "source": [
    "sentences = \"Hello My name is Alice I live in Seoul\"\n",
    "result = sent_tokenize(sentences)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, My name is Alice.', 'How are you?', 'I live in Seoul.']\n"
     ]
    }
   ],
   "source": [
    "sentences = \"Hello, My name is Alice. How are you? I live in Seoul.\"\n",
    "result = sent_tokenize(sentences)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello',\n",
      " '.',\n",
      " 'My',\n",
      " 'name',\n",
      " 'is',\n",
      " 'Alice',\n",
      " '.',\n",
      " 'I',\n",
      " 'live',\n",
      " 'in',\n",
      " 'Seoul',\n",
      " '.']\n"
     ]
    }
   ],
   "source": [
    "sentences = \"Hello. My name is Alice. I live in Seoul.\"\n",
    "result = word_tokenize(sentences)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Result of word_tokenize ====\n",
      "['All',\n",
      " 'she',\n",
      " 'talking',\n",
      " 'bout',\n",
      " 'is',\n",
      " 'come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'me',\n",
      " 'for',\n",
      " 'once',\n",
      " 'Come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'me',\n",
      " 'for',\n",
      " 'once',\n",
      " 'You',\n",
      " 'do',\n",
      " \"n't\",\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me',\n",
      " ',',\n",
      " 'you',\n",
      " 'do',\n",
      " \"n't\",\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me',\n",
      " 'All',\n",
      " 'she',\n",
      " 'ever',\n",
      " 'say',\n",
      " 'is',\n",
      " 'come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'me',\n",
      " 'for',\n",
      " 'once',\n",
      " 'Come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'me',\n",
      " 'for',\n",
      " 'once',\n",
      " 'You',\n",
      " 'do',\n",
      " \"n't\",\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me',\n",
      " ',',\n",
      " 'you',\n",
      " 'do',\n",
      " \"n't\",\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me']\n",
      "\n",
      "\n",
      "==== Result of wordpunct_tokenize ====\n",
      "['All',\n",
      " 'she',\n",
      " 'talking',\n",
      " 'bout',\n",
      " 'is',\n",
      " 'come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'me',\n",
      " 'for',\n",
      " 'once',\n",
      " 'Come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'me',\n",
      " 'for',\n",
      " 'once',\n",
      " 'You',\n",
      " 'don',\n",
      " \"'\",\n",
      " 't',\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me',\n",
      " ',',\n",
      " 'you',\n",
      " 'don',\n",
      " \"'\",\n",
      " 't',\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me',\n",
      " 'All',\n",
      " 'she',\n",
      " 'ever',\n",
      " 'say',\n",
      " 'is',\n",
      " 'come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'me',\n",
      " 'for',\n",
      " 'once',\n",
      " 'Come',\n",
      " 'and',\n",
      " 'see',\n",
      " 'me',\n",
      " 'for',\n",
      " 'once',\n",
      " 'You',\n",
      " 'don',\n",
      " \"'\",\n",
      " 't',\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me',\n",
      " ',',\n",
      " 'you',\n",
      " 'don',\n",
      " \"'\",\n",
      " 't',\n",
      " 'ever',\n",
      " 'come',\n",
      " 'to',\n",
      " 'me']\n"
     ]
    }
   ],
   "source": [
    "sentences = \"\"\"\n",
    "All she talking bout is come and see me for once\n",
    "Come and see me for once\n",
    "You don't ever come to me, you don't ever come to me\n",
    "All she ever say is come and see me for once\n",
    "Come and see me for once\n",
    "You don't ever come to me, you don't ever come to me\n",
    "\"\"\"\n",
    "\n",
    "word_tokenize_result = word_tokenize(sentences)\n",
    "wordpunct_tokenize_result = wordpunct_tokenize(sentences)\n",
    "\n",
    "print(\"==== Result of word_tokenize ====\")\n",
    "pprint(word_tokenize_result)\n",
    "print(\"\\n\")\n",
    "print(\"==== Result of wordpunct_tokenize ====\")\n",
    "pprint(wordpunct_tokenize_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part-of-speech (POS) tagging\n",
    "- nltk.pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('All', 'DT'),\n",
      " ('she', 'PRP'),\n",
      " ('talking', 'VBG'),\n",
      " ('bout', 'NN'),\n",
      " ('is', 'VBZ'),\n",
      " ('come', 'VBN'),\n",
      " ('and', 'CC'),\n",
      " ('see', 'VB'),\n",
      " ('me', 'PRP'),\n",
      " ('for', 'IN'),\n",
      " ('once', 'RB'),\n",
      " ('Come', 'NNP'),\n",
      " ('and', 'CC'),\n",
      " ('see', 'VB'),\n",
      " ('me', 'PRP'),\n",
      " ('for', 'IN'),\n",
      " ('once', 'RB'),\n",
      " ('You', 'PRP'),\n",
      " ('don', 'VBP'),\n",
      " (\"'\", \"''\"),\n",
      " ('t', 'JJ'),\n",
      " ('ever', 'RB'),\n",
      " ('come', 'VBP'),\n",
      " ('to', 'TO'),\n",
      " ('me', 'PRP'),\n",
      " (',', ','),\n",
      " ('you', 'PRP'),\n",
      " ('don', 'VBP'),\n",
      " (\"'\", \"''\"),\n",
      " ('t', 'JJ'),\n",
      " ('ever', 'RB'),\n",
      " ('come', 'VBP'),\n",
      " ('to', 'TO'),\n",
      " ('me', 'PRP'),\n",
      " ('All', 'PDT'),\n",
      " ('she', 'PRP'),\n",
      " ('ever', 'RB'),\n",
      " ('say', 'VBP'),\n",
      " ('is', 'VBZ'),\n",
      " ('come', 'JJ'),\n",
      " ('and', 'CC'),\n",
      " ('see', 'VB'),\n",
      " ('me', 'PRP'),\n",
      " ('for', 'IN'),\n",
      " ('once', 'RB'),\n",
      " ('Come', 'NNP'),\n",
      " ('and', 'CC'),\n",
      " ('see', 'VB'),\n",
      " ('me', 'PRP'),\n",
      " ('for', 'IN'),\n",
      " ('once', 'RB'),\n",
      " ('You', 'PRP'),\n",
      " ('don', 'VBP'),\n",
      " (\"'\", \"''\"),\n",
      " ('t', 'JJ'),\n",
      " ('ever', 'RB'),\n",
      " ('come', 'VBP'),\n",
      " ('to', 'TO'),\n",
      " ('me', 'PRP'),\n",
      " (',', ','),\n",
      " ('you', 'PRP'),\n",
      " ('don', 'VBP'),\n",
      " (\"'\", \"''\"),\n",
      " ('t', 'JJ'),\n",
      " ('ever', 'RB'),\n",
      " ('come', 'VBP'),\n",
      " ('to', 'TO'),\n",
      " ('me', 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "sentences = \"\"\"\n",
    "All she talking bout is come and see me for once\n",
    "Come and see me for once\n",
    "You don't ever come to me, you don't ever come to me\n",
    "All she ever say is come and see me for once\n",
    "Come and see me for once\n",
    "You don't ever come to me, you don't ever come to me\n",
    "\"\"\"\n",
    "\n",
    "pos_result = nltk.pos_tag(nltk.tokenize.wordpunct_tokenize(sentences))\n",
    "pprint(pos_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalization\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the women run in the fog pass bunni work as comput scientist .\n",
      "['the', 'women', 'run', 'in', 'the', 'fog', 'pass', 'bunni', 'work', 'as', 'comput', 'scientist', '.']\n",
      "the wom run in the fog pass bunny work as comput sci .\n",
      "['the', 'wom', 'run', 'in', 'the', 'fog', 'pass', 'bunny', 'work', 'as', 'comput', 'sci', '.']\n",
      "the women run in the fog pass bunni work as comput scientist .\n",
      "['the', 'women', 'run', 'in', 'the', 'fog', 'pass', 'bunni', 'work', 'as', 'comput', 'scientist', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "text = list(nltk.word_tokenize(\"The women running in the fog passed bunnies working as computer scientists.\"))\n",
    "\n",
    "snowball = SnowballStemmer('english')\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for stemmer in (snowball, lancaster, porter):\n",
    "    stemmed_text = [stemmer.stem(t) for t in text]\n",
    "    print(\" \".join(stemmed_text))\n",
    "    print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman running in the fog passed bunny working a computer scientist .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Note: use part of speech tag, we'll see this in machine learning! \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(t) for t in text]\n",
    "print(\" \".join(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eagle', 'fly', 'midnight']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "## Module constants\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "stopwords   = set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "\n",
    "def tagwn(tag):\n",
    "    \"\"\"\n",
    "    Returns the WordNet tag from the Penn Treebank tag.\n",
    "    \"\"\"\n",
    "\n",
    "    return {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "    }.get(tag[0], wn.NOUN)\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    for token, tag in nltk.pos_tag(nltk.wordpunct_tokenize(text)):\n",
    "        #if you're going to do part of speech tagging, do it here\n",
    "        token = token.lower()\n",
    "        if token in stopwords or token in punctuation:\n",
    "            continue\n",
    "        token = lemmatizer.lemmatize(token, tagwn(tag))\n",
    "        yield token\n",
    "\n",
    "print(list(normalize(\"The eagle ! up a flies at midnight.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'down', 'herself', \"aren't\", 'with', 'on', \"couldn't\", 'we', 'couldn', 'each', 'other', 'what', 'after', \"wasn't\", 'before', 'into', 'been', 'in', 'out', \"didn't\", 'then', 'few', 'yourself', 'weren', \"mustn't\", 'wouldn', 'did', 'itself', 'you', 'again', \"haven't\", 'from', 'my', 'ours', \"don't\", 'of', 'how', 'she', \"won't\", 'against', 'than', 'doing', 'by', \"hadn't\", 'both', 'isn', 's', 'were', 'wasn', 'won', 'those', 'under', 'more', 'just', 'they', 'him', 'be', 'them', 'once', 'having', 'an', 'until', 'ain', 'all', 'this', 'too', 'which', 'here', 'yourselves', \"shouldn't\", 'll', 'during', \"it's\", 't', \"you'd\", 'm', 'mustn', 'me', 'shan', 'has', \"mightn't\", 'themselves', 'don', 'he', \"you'll\", 'theirs', 're', 've', 'no', 'our', 'such', 'aren', 'further', 'if', \"she's\", \"you've\", 'while', \"needn't\", 'there', 'doesn', 'have', 'some', 'is', 'didn', \"wouldn't\", 'to', \"that'll\", 'shouldn', 'hadn', 'are', 'the', \"you're\", 'was', 'very', 'or', \"should've\", 'had', 'through', 'but', 'being', 'below', \"hasn't\", 'above', 'most', 'their', 'mightn', 'does', 'any', 'your', 'do', 'hers', \"isn't\", 'at', 'only', 'so', 'nor', 'because', 'her', 'who', 'should', 'up', 'over', 'his', 'ourselves', 'not', 'o', 'can', 'as', 'between', 'am', \"shan't\", \"weren't\", 'yours', 'haven', 'these', 'off', 'and', 'that', 'needn', 'own', 'ma', 'its', 'myself', 'for', 'same', 'why', \"doesn't\", 'y', 'now', 'about', 'will', 'himself', 'a', 'hasn', 'where', 'whom', 'it', 'i', 'when', 'd'}\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Named-entity recognition (NER)\n",
    "- Maximum entropy based NER\n",
    "- Stanford NER packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  LG/NNP\n",
      "  electronics/NNS\n",
      "  released/VBD\n",
      "  the/DT\n",
      "  smart/JJ\n",
      "  phone/NN\n",
      "  'G6/POS\n",
      "  '/''\n",
      "  in/IN\n",
      "  (GPE April/NNP)\n",
      "  ,/,\n",
      "  2017/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"LG electronics released the smart phone 'G6' in April, 2017.\"\n",
    "print(nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Stanford NER packages: https://nlp.stanford.edu/software/CRF-NER.shtml#Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORGANIZATION] Samsung\n",
      "[O] electronics\n",
      "[ORGANIZATION] Microsoft\n",
      "[O] research\n",
      "[ORGANIZATION] GE\n",
      "[ORGANIZATION] LG\n",
      "[ORGANIZATION] Baidu\n",
      "[ORGANIZATION] Amazon\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "stanford_data = 'C:\\\\Users\\\\user\\\\stanford-ner-2018-10-16\\\\stanford-ner-2018-10-16\\\\classifiers\\\\english.all.3class.distsim.crf.ser.gz'\n",
    "stanford_jar =  'C:\\\\Users\\\\user\\\\stanford-ner-2018-10-16\\\\stanford-ner-2018-10-16\\\\stanford-ner-3.9.2.jar'\n",
    "\n",
    "text = \"Samsung electronics Microsoft research GE LG Baidu Amazon\"\n",
    "st = StanfordNERTagger(stanford_data, stanford_jar, 'utf-8')\n",
    "for i in st.tag(text.split()):\n",
    "    print('[' + i[1] + '] ' + i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parsing\n",
    "- Parsing using a grammar\n",
    "- StanfordParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.grammar.CFG.fromstring(\"\"\"\n",
    "\n",
    "S -> NP PUNCT | NP\n",
    "NP -> N N | ADJP NP | DET N | DET ADJP\n",
    "ADJP -> ADJ NP | ADJ N\n",
    "\n",
    "DET -> 'an' | 'the' | 'a' | 'that'\n",
    "N -> 'airplane' | 'runway' | 'lawn' | 'chair' | 'person' \n",
    "ADJ -> 'red' | 'slow' | 'tired' | 'long'\n",
    "PUNCT -> '.'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (DET the) (ADJP (ADJ long) (N runway))))\n"
     ]
    }
   ],
   "source": [
    "def parse(sent):\n",
    "    sent = sent.lower()\n",
    "    parser = nltk.parse.ChartParser(grammar)\n",
    "    for p in parser.parse(nltk.word_tokenize(sent)):\n",
    "        yield p \n",
    "\n",
    "        \n",
    "for tree in parse(\"the long runway\"): \n",
    "    tree.pprint()\n",
    "#     tree[0].draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Stanford Parser packages: https://nlp.stanford.edu/software/lex-parser.shtml#Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (DT The) (NN man))\n",
      "    (VP\n",
      "      (VBD hit)\n",
      "      (NP (DT the) (NN building))\n",
      "      (PP (IN with) (NP (DT the) (NN baseball) (NN bat))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "stanford_model = 'C:\\\\Users\\\\user\\\\stanford-parser-full-2018-10-17\\\\stanford-parser-full-2018-10-17\\\\stanford-parser-3.9.2-models.jar'\n",
    "stanford_jar = 'C:\\\\Users\\\\user\\\\stanford-parser-full-2018-10-17\\\\stanford-parser-full-2018-10-17\\\\stanford-parser.jar'\n",
    "\n",
    "st = StanfordParser(stanford_model, stanford_jar)\n",
    "sent = \"The man hit the building with the baseball bat.\"\n",
    "for tree in st.parse(nltk.wordpunct_tokenize(sent)):\n",
    "    tree.pprint()\n",
    "#     tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
